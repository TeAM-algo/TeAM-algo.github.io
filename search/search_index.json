{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TeAM: Text Approximation Matching Al Idrissou (UvA/Huygens) & Veruska Zamborlini (UvA) Developed in the context of the Golden Agents project \u00b6","title":"<div align=\"center\"> TeAM: Text Approximation Matching <div align=\"center\" style=\"font-size:1.4vw\"> <span style=\"color:blue\">  Al Idrissou (UvA/Huygens) & Veruska Zamborlini (UvA) </span> <br> <div align=\"center\" style=\"font-size:1.1vw\"><span style=\"color:purple\"> Developed in the context of the Golden Agents project</div> </div></div>"},{"location":"#team-text-approximation-matching-al-idrissou-uvahuygens-veruska-zamborlini-uva-developed-in-the-context-of-the-golden-agents-project","text":"","title":" TeAM: Text Approximation Matching    Al Idrissou (UvA/Huygens) &amp; Veruska Zamborlini (UvA)    Developed in the context of the Golden Agents project "},{"location":"01.Introduction/","text":"1. Introduction \u00b6 The Amsterdam\u2019s city archives (SAA) possesses physical handwritten inventories records where a record may be for example an inventory of goods (paintings, prints, sculpture, furniture, porcelain, etc.) owned by an Amsterdamer and mentioned in a last will. Interested in documenting the ownership of paintings from the 17 th century, the Yale University Professor John Michael Montias compiled a database by transcribing 1280 physical handwritten inventories (scattered in the Netherlands) of goods. Now that a number of these physical inventories have been digitised using handwriting recognition, one of the goals of the Golden Agent project is to identify Montias\u2019 transcriptions of painting selections within the digitised inventories. This problem can be generically reformulated as, given a source-segment database (e.g. Montias DB) and a target-segment database (e.g. SAA) , find the best similar target segment for each source segment . The problem introduced here relates to the approximation of the relevance of a document to a query. Such approximation can be done using lexical similarity (word level similarity), semantic similarity or hybrid similarities. In this work, given the problem at hand, the focus is rather on the lexical similarity.","title":"1. Introduction"},{"location":"01.Introduction/#1-introduction","text":"The Amsterdam\u2019s city archives (SAA) possesses physical handwritten inventories records where a record may be for example an inventory of goods (paintings, prints, sculpture, furniture, porcelain, etc.) owned by an Amsterdamer and mentioned in a last will. Interested in documenting the ownership of paintings from the 17 th century, the Yale University Professor John Michael Montias compiled a database by transcribing 1280 physical handwritten inventories (scattered in the Netherlands) of goods. Now that a number of these physical inventories have been digitised using handwriting recognition, one of the goals of the Golden Agent project is to identify Montias\u2019 transcriptions of painting selections within the digitised inventories. This problem can be generically reformulated as, given a source-segment database (e.g. Montias DB) and a target-segment database (e.g. SAA) , find the best similar target segment for each source segment . The problem introduced here relates to the approximation of the relevance of a document to a query. Such approximation can be done using lexical similarity (word level similarity), semantic similarity or hybrid similarities. In this work, given the problem at hand, the focus is rather on the lexical similarity.","title":" 1. Introduction "},{"location":"02.RelatedWork/","text":"2. Related Work \u00b6 Gensim \u00b6 Annif is a Subject Indexer tool for helping librarians index books in general. However, because the tool uses a number of interesting libraries ( NLTK and Voikko for tokennpzation , stemming , lemmatising or Gensim for computing TF-IDF ) for tasks perfectly fitting with reaching our objective, we investigate it. Two observations follow from our investigations: (i) the library is fast but (ii) the approach for similarity is limited to the extraction of documents for which a number of terms exactly much terms in the query.","title":"2. Related Workd"},{"location":"02.RelatedWork/#2-related-work","text":"","title":" 2. Related Work "},{"location":"02.RelatedWork/#gensim","text":"Annif is a Subject Indexer tool for helping librarians index books in general. However, because the tool uses a number of interesting libraries ( NLTK and Voikko for tokennpzation , stemming , lemmatising or Gensim for computing TF-IDF ) for tasks perfectly fitting with reaching our objective, we investigate it. Two observations follow from our investigations: (i) the library is fast but (ii) the approach for similarity is limited to the extraction of documents for which a number of terms exactly much terms in the query.","title":"Gensim"},{"location":"03.Terminology/","text":"3. Terminology \u00b6 We define a text-document as a digital piece of written matter that provides information and is saved in one or multiple files. Example of documents are newspaper, books, book titles or articles. An entire document of one or more files can be partitioned in smaller pieces of text that we denote as segments . Example of segments are lines, sentences, paragraphs, sections among others. We use the term hit to denote that a candidate-term is found for a query-term.","title":"3. Terminology"},{"location":"03.Terminology/#3-terminology","text":"We define a text-document as a digital piece of written matter that provides information and is saved in one or multiple files. Example of documents are newspaper, books, book titles or articles. An entire document of one or more files can be partitioned in smaller pieces of text that we denote as segments . Example of segments are lines, sentences, paragraphs, sections among others. We use the term hit to denote that a candidate-term is found for a query-term.","title":" 3. Terminology "},{"location":"04.TestData/","text":"4. Test Data \u00b6 The data used for the matching experiments and described in the next subsections are provided by the Golden Agents project and the Getty. 4.1 Target Data \u00b6 The SAA Inventory Documents database is used in this work as the target data. A number of handwritten inventory documents have already undergone the digitising process amid the SAA\u2019s goal to computerise its data. As the sole automated Handwritten Text Recognition (HTR) of inventory documents is not of sufficient quality, the current data in use has been curated. Nevertheless, it still contains some irregularities. These include among others, (1) segments that are broken sentences , which (2) sometimes come out of order , (3) the appearance of the symbol ^ in \u201cEen ^ besgie\u201d denoting the inability of the HTR to recognise a handwritten term. An important note regarding the broken segments. One would have expected the curated computerised documents to come free of broken segments. However, because the goal is to have a transcript faithful to the original (instead of outputting correct sentence segments), the broken sentences are again present in the curated version as observed in the original handwritten document. SAA Inventory documents sample ------------------------------------------------------------------------------------------------------------------------------------------- SAA-item-htr-uri text ------------------------------------------------------------------------------------------------------------------------------------------- https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l1 Inventaris ende Specificatie https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l2 van allen den goederen nagelaten https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l3 bij zal. Maria koerten sulcx https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l4 zij die met haer man Niclaes felt https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l5 int gemeen beseten ende metter https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l6 dood deser werelt ontruijmt https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l7 ende nagelaten heeft. https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l8 Eerstelijck den huijsraet, Imboel https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l9 klederen, kleijnodien ende Juwelen https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l10 getaxeert door Annetje hendricx en Susannetje https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l11 Anthonis gesworen Taxeersters deser stede https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l12 tot alsulcken prijse als volcht https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l14 Een wiegh met een baecker mat, ende https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l15 een matstoel, een bakermand ende https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l16 4 https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l17 eenige kleijne mantjes https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l18 Een scharfbort met een doos ende eenigh https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l19 -10 https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l20 withoutwerck en rommeling https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l21 Een ijsere rooster, brandijser en een https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l22 2 4.2 Source data \u00b6 In order to document the ownership of painting mentioned in inventories, Montias and others transcribe about 1280 inventories in total. This database of Transcripts (MDB) is therefore used in this work for as the source data for testing purpose. During the task of transcribing, a number of decision was made. For example, The particular way of enumerating an item such as \u201cNo. 21\u201d in the digitised SAA is simply discarded by Montias. Montias included its own representation of the value of an item, for example \u2026 gilders is represented as \u201c2:10:\u2013\u201d . Abbreviations do not appear consistently in both transcripts. \u201cNo. 39. L. vrouwe beeldekens\u201d in SAA for example appears as \u201cLieve-Vrouwe-beeldekens 1:10:\u2013\u201c in MDB, but also turned into one word by the use of hyphens. A number of spelling variations are also observed. For example, the following terms \u201cDrij, water hontjes, prentebortjes, van de\u201d in SAA are transcribed in MDB as \u201cDry , waterhontjes, print borts, vande\u201d Other observations come as modifications and repatitions . For example, in the SAA, \u201c5 alabaster bortjes. Een affneming van \u2018t kruys\u201d become \u201c Vier Albastarde bortjes met vergulde lijstjes. Een affneming vant kruijs\u201d in MDB. Observed that the number 5 has been mistakenly reported as the word vier instead of vijf . Additionaly, the single item \u201c5 alabaster bortjes\u201d in SAA appears five times in MDB and sometimes with mistake: \u201cVier Albastarde bortjes met vergulde lijstjes.\u201d Montias data transcript sample. --------------------------------------------------------------------------------------------------------- Getty - Frick - item - uri transcription --------------------------------------------------------------------------------------------------------- http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0001 Een schilderij van de Samaritaen http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0002 Een stuck schilderij van de liefde http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0003 De salvingh Christi http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0004 Een zeevaertje http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0005 Een landschap met een vergulde lijst http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0006 Een landschap http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0007 Een gerecht http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0008 Eenich geselschap van boeren http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0009 Een landschap http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0010 Een landschap http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0011 De liefde http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0012 Een landschapje http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0013 Mede een landschap http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0014 Eenich geberchte http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0015 Salvator http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0016 Een oud besgie http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0017 Een stuckje , gemaeckt bij Potuyl http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0018 De ( N . B . verbeterd uit : Een ) geboorte http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0019 Noch een stuckje , van Potuyl http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0020 Een munnick ende een bagijn 4.3 Foreseen matching issues \u00b6 All the afore mentioned irregularities leads to some extra difficulties in the already hard task of segment matching. Below, we categorise these foreseen problems as major versus minor issues. At the moment, the main issue comes with the broken sentences and ties. Ties occur when the same item occurs several times in the target data. Major \u00b6 Broken sentences Abbreviations Combined words Numbers to words Terms not recognised by HTR Repetitions (same query posed more than once) Ties (multiple answers for the same query) Minor \u00b6 Spelling variations Small mistakes Normalisation Data cleaning: remove terms that are added or disregarded by Montias. Repetitions & Ties \u00b6 Duplicates in the target data appear for good reasons. A named item ( \u201cEen landschap\u201d ) can occur more than once in the same house (same inventory) or in multiple houses (multiple inventories). In the source data, the same phenomenon can occur for the same reason (correct transcript) or due to subjective interventions, for example reporting the segment 5 times although it appears one time in the original document as a set instead. Ideally, we would like to be able to remove those subjective duplicates so that we do not search for the same target several times. Observe that computing the same question/answer several times may result in a misleading evaluation of the algorithm. These interventions make it difficult to remove duplicates in the source as there is little or no information on which to base the inference of VALID versus INVALID duplicates. So, although occurring three times ( A1:Een landschap, A2:Een landschap, A3:Een landschap ), A2:Een landschap can not be linked to a randomly selected exact match from a set of segments in the target, which are similar to the source ( B1:Een landschap, B2:Een landschap, B3:Een landschap ). In other words, A2:Een landschap can only be matched to A2:Een landschap for the sake of example. This illustration indicates that, in practice, for a given source transcript there is only one correct answer in the target data.","title":"4. Test Data"},{"location":"04.TestData/#4-test-data","text":"The data used for the matching experiments and described in the next subsections are provided by the Golden Agents project and the Getty.","title":" 4. Test Data "},{"location":"04.TestData/#41-target-data","text":"The SAA Inventory Documents database is used in this work as the target data. A number of handwritten inventory documents have already undergone the digitising process amid the SAA\u2019s goal to computerise its data. As the sole automated Handwritten Text Recognition (HTR) of inventory documents is not of sufficient quality, the current data in use has been curated. Nevertheless, it still contains some irregularities. These include among others, (1) segments that are broken sentences , which (2) sometimes come out of order , (3) the appearance of the symbol ^ in \u201cEen ^ besgie\u201d denoting the inability of the HTR to recognise a handwritten term. An important note regarding the broken segments. One would have expected the curated computerised documents to come free of broken segments. However, because the goal is to have a transcript faithful to the original (instead of outputting correct sentence segments), the broken sentences are again present in the curated version as observed in the original handwritten document. SAA Inventory documents sample ------------------------------------------------------------------------------------------------------------------------------------------- SAA-item-htr-uri text ------------------------------------------------------------------------------------------------------------------------------------------- https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l1 Inventaris ende Specificatie https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l2 van allen den goederen nagelaten https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l3 bij zal. Maria koerten sulcx https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l4 zij die met haer man Niclaes felt https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l5 int gemeen beseten ende metter https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l6 dood deser werelt ontruijmt https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l7 ende nagelaten heeft. https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l8 Eerstelijck den huijsraet, Imboel https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l9 klederen, kleijnodien ende Juwelen https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l10 getaxeert door Annetje hendricx en Susannetje https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l11 Anthonis gesworen Taxeersters deser stede https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l12 tot alsulcken prijse als volcht https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l14 Een wiegh met een baecker mat, ende https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l15 een matstoel, een bakermand ende https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l16 4 https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l17 eenige kleijne mantjes https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l18 Een scharfbort met een doos ende eenigh https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l19 -10 https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l20 withoutwerck en rommeling https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l21 Een ijsere rooster, brandijser en een https://archief.amsterdam/inventarissen/inventaris/5075.nl.html#A16098000031r1l22 2","title":" 4.1 Target Data "},{"location":"04.TestData/#42-source-data","text":"In order to document the ownership of painting mentioned in inventories, Montias and others transcribe about 1280 inventories in total. This database of Transcripts (MDB) is therefore used in this work for as the source data for testing purpose. During the task of transcribing, a number of decision was made. For example, The particular way of enumerating an item such as \u201cNo. 21\u201d in the digitised SAA is simply discarded by Montias. Montias included its own representation of the value of an item, for example \u2026 gilders is represented as \u201c2:10:\u2013\u201d . Abbreviations do not appear consistently in both transcripts. \u201cNo. 39. L. vrouwe beeldekens\u201d in SAA for example appears as \u201cLieve-Vrouwe-beeldekens 1:10:\u2013\u201c in MDB, but also turned into one word by the use of hyphens. A number of spelling variations are also observed. For example, the following terms \u201cDrij, water hontjes, prentebortjes, van de\u201d in SAA are transcribed in MDB as \u201cDry , waterhontjes, print borts, vande\u201d Other observations come as modifications and repatitions . For example, in the SAA, \u201c5 alabaster bortjes. Een affneming van \u2018t kruys\u201d become \u201c Vier Albastarde bortjes met vergulde lijstjes. Een affneming vant kruijs\u201d in MDB. Observed that the number 5 has been mistakenly reported as the word vier instead of vijf . Additionaly, the single item \u201c5 alabaster bortjes\u201d in SAA appears five times in MDB and sometimes with mistake: \u201cVier Albastarde bortjes met vergulde lijstjes.\u201d Montias data transcript sample. --------------------------------------------------------------------------------------------------------- Getty - Frick - item - uri transcription --------------------------------------------------------------------------------------------------------- http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0001 Een schilderij van de Samaritaen http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0002 Een stuck schilderij van de liefde http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0003 De salvingh Christi http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0004 Een zeevaertje http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0005 Een landschap met een vergulde lijst http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0006 Een landschap http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0007 Een gerecht http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0008 Eenich geselschap van boeren http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0009 Een landschap http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0010 Een landschap http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0011 De liefde http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0012 Een landschapje http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0013 Mede een landschap http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0014 Eenich geberchte http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0015 Salvator http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0016 Een oud besgie http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0017 Een stuckje , gemaeckt bij Potuyl http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0018 De ( N . B . verbeterd uit : Een ) geboorte http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0019 Noch een stuckje , van Potuyl http : // goldenagents . org / uv a / SAA / Inventory / Item / N -2097 _ 0020 Een munnick ende een bagijn","title":" 4.2 Source data "},{"location":"04.TestData/#43-foreseen-matching-issues","text":"All the afore mentioned irregularities leads to some extra difficulties in the already hard task of segment matching. Below, we categorise these foreseen problems as major versus minor issues. At the moment, the main issue comes with the broken sentences and ties. Ties occur when the same item occurs several times in the target data.","title":" 4.3 Foreseen matching issues "},{"location":"04.TestData/#major","text":"Broken sentences Abbreviations Combined words Numbers to words Terms not recognised by HTR Repetitions (same query posed more than once) Ties (multiple answers for the same query)","title":"Major"},{"location":"04.TestData/#minor","text":"Spelling variations Small mistakes Normalisation Data cleaning: remove terms that are added or disregarded by Montias.","title":"Minor"},{"location":"04.TestData/#repetitions-ties","text":"Duplicates in the target data appear for good reasons. A named item ( \u201cEen landschap\u201d ) can occur more than once in the same house (same inventory) or in multiple houses (multiple inventories). In the source data, the same phenomenon can occur for the same reason (correct transcript) or due to subjective interventions, for example reporting the segment 5 times although it appears one time in the original document as a set instead. Ideally, we would like to be able to remove those subjective duplicates so that we do not search for the same target several times. Observe that computing the same question/answer several times may result in a misleading evaluation of the algorithm. These interventions make it difficult to remove duplicates in the source as there is little or no information on which to base the inference of VALID versus INVALID duplicates. So, although occurring three times ( A1:Een landschap, A2:Een landschap, A3:Een landschap ), A2:Een landschap can not be linked to a randomly selected exact match from a set of segments in the target, which are similar to the source ( B1:Een landschap, B2:Een landschap, B3:Een landschap ). In other words, A2:Een landschap can only be matched to A2:Een landschap for the sake of example. This illustration indicates that, in practice, for a given source transcript there is only one correct answer in the target data.","title":"Repetitions &amp; Ties"},{"location":"05.EvaluationApproach/","text":"5. Evaluation Approach \u00b6 For evaluation purposes, a manually created ground truth (by Veruska Zamborlini) is provided as matching pairs between source and target, denoting which segment of the target has been transcribed in the source data. The target data (SAA) is composed of 4179 segments from 4 inventories while the source data (MDB) is of 200 segments that represent a partial transcription of the 4 inventories . Both data include segment duplicates discussed above as repetitions or ties. In principle, for a given query-segment (Montias transcript), there exists only a single match in the target. However, because the target data is composed of segments from multiple inventories, segments with the exact same text can be found in the same inventory or in multiple inventories. Furthermore, a segment can also be split into multiple segments. This leads to finding more than one target segment for certain query-segment. This issue is addressed in the matching strategy. 5.1 The Evaluation Strategy \u00b6 Broken Segments . When a segment is split into multiple segments (multiple lines in this context), the ground truth provides multiple segments as an answer, which highlights the existence of a wrong segment. This means that predicting any of the ground truth answers is correct. However, if multiple answers are given by the predictor for that query, then all predictions must be correct. For example, if the answer for a query-segment A is a set of wrongly broken segments {E, F, G}, predicting any element from the set is correct but predicting {A, F, C} is incorrect because no wrong element should be included in the prediction set. Duplicated Queries. Consistency dictates that, for the same query asked several times, the same answer should be returned unless new information is available. For this reason, duplicates (segments with the exact same text) are removed from the query data. This brings the source query from 200 to 172. Like the broken segments, targets that have the exact same text should be returned as answers as a set. We denote this set as true ties .","title":"5.Evaluation Approach"},{"location":"05.EvaluationApproach/#5-evaluation-approach","text":"For evaluation purposes, a manually created ground truth (by Veruska Zamborlini) is provided as matching pairs between source and target, denoting which segment of the target has been transcribed in the source data. The target data (SAA) is composed of 4179 segments from 4 inventories while the source data (MDB) is of 200 segments that represent a partial transcription of the 4 inventories . Both data include segment duplicates discussed above as repetitions or ties. In principle, for a given query-segment (Montias transcript), there exists only a single match in the target. However, because the target data is composed of segments from multiple inventories, segments with the exact same text can be found in the same inventory or in multiple inventories. Furthermore, a segment can also be split into multiple segments. This leads to finding more than one target segment for certain query-segment. This issue is addressed in the matching strategy.","title":" 5. Evaluation Approach "},{"location":"05.EvaluationApproach/#51-the-evaluation-strategy","text":"Broken Segments . When a segment is split into multiple segments (multiple lines in this context), the ground truth provides multiple segments as an answer, which highlights the existence of a wrong segment. This means that predicting any of the ground truth answers is correct. However, if multiple answers are given by the predictor for that query, then all predictions must be correct. For example, if the answer for a query-segment A is a set of wrongly broken segments {E, F, G}, predicting any element from the set is correct but predicting {A, F, C} is incorrect because no wrong element should be included in the prediction set. Duplicated Queries. Consistency dictates that, for the same query asked several times, the same answer should be returned unless new information is available. For this reason, duplicates (segments with the exact same text) are removed from the query data. This brings the source query from 200 to 172. Like the broken segments, targets that have the exact same text should be returned as answers as a set. We denote this set as true ties .","title":"5.1 The Evaluation Strategy"},{"location":"06.Algorithms/","text":"6. Algorithms \u00b6 Given source-segment and target-segment databases, we have designed and developed a library that outputs a number of candidates (of choice) from the target data for each source-segment entry. The TeAM algorithm allows for a number of other choices including among athers boosting the strength of a matched term using its lemma or soundex and/or matching possible abbreviations. To achieve this, we successfully derive benefit from existing libraries such as panda , fuzzywuzzy , spacy and ftfy . Moreover, we compare our approach with a rather simpler one mostly based on Gensim . Hereby we explain them both. 6.1 Gensim+ \u00b6 In order to have a baseline for comparing the TeAM approach, we chose the \u2018of the shelf\u2019 library \u2018Gensim\u2019 proposed for text search. We implement it first by strictly following the instructions presented in here . Since it is meant for exact term-match and it is, therefore, unable to deal with non existing (exact) terms in the target data, the results at first appear quite poor for our complex data. One way to help dealing with such issue is to convert the source and target data into their respective stems . This indeed improves the results since the matches are now approximated via the stems. Still, this does not account for misspellings or spelling variations. An attempt to address the latter issue led us to multiply the original query into a number of reconstructed queries from which the best result is selected. To reconstruct a query, n approximations are computed for each stemmed term in the query. From this, new queries are generated (term combinations by n-ary Cartesian product). For example, a query of 5 terms will result in 32 new queries if 2 candidates are found for each term. This obviously does not scale in the event of (i) big queries and/or (ii) large candidates (even 2). However, the use of a single candidate does scale and provides at list one alternative to terms that do not exist in the target data, meaning that misspell or spelling variations are now included to a certain extent. To summarise, using the Gensim library we have developed and implemented a text matching approach that now includes the search of terms that do not exist in the target data. 6.2 TeAM \u00b6 This section presents TeAM, the proposed algorithm for text matching. First its parameters are described, then its 4 macro steps are explained. Function Definition \u00b6 Function Definition results = TeAM.run( queries=montias_data, target=inventories, stop_words=dutch_stop_word, language='nl', max_candidates=5, boost=True, preprocess=True, normalise=True, find_abbreviated=True, logarithm=False, remove_number=True, no_ties=True, x_best_tf_idfs=50, bests=1, sample=0) Parameter Description :param queries : The source data. It can be a plain text or a list of text segments. :param target : The target data. It can be a plain text or a list of text segments. :param stop_words : List of current terms that should not be indexed. :param language : A string value determining the language (e.g en, de...) in which the source and target data are written. :param max_candidates : The number of candidates terms that can be return for a given query-token :param boost : A boolean value for deciding whether to boost the strength of a matched candidate below a low-bound of 75 using the sound and lemma of the terms. :param preprocess : A boolean value for deciding whether to apply the fix_text_segment function for fixing inconsistencies and glitches in the source and target data :param normalise_text : Defaulted to [False], it is applied to the dutch written source and target data to normalise certain characters. For example, 'sch' and 'kx' will become respectfully 'see' and 'xx'. :param normalize_number : Here too, It is also used to determine whether to convert numbers to words using num2words. :param logarithm : Defaulted to [False], it uses the log function in computing tf-idf if set to [True] :param find_abbreviated : Defaulted to True, it ensures that abbreviated terms are recognised, tagged and that, possible terms for which the abbreviated term can stand for within the data are looked for. :param remove_number : Defaulted to True, it ensures that numbers are not indexed :param no_ties : Defaulted to [True].. :param x_best_tf_idfs : Defaulted to 50. The tf_idf vector enabled us to detect potential segment candidates. The integer value indicates the number of best candidates to look into. :param bests : By default, for a given query, the predicted best is returned (bests=1). :param sample : If the integer value is greater than 0, it prints panda tables of the index source, target and tf_idf vector of the target data :return : a dictionary of list or results for each query segment Overview \u00b6 Steps Overview def run ( queries : ( str , list ), target : ( str , list ), stop_words : list , language : str = 'nl' , preprocess : bool = True , normalise_text : bool = True , normalize_number : bool = True , remove_number = True , find_abbreviated = True , no_ties : bool = True , max_candidates : int = 5 , boost_candidates : bool = True , logarithm = False , n_best_tf_idfs : int = 50 , n_bests : int = 1 ): global nl , en nl , en = nl_core_news_sm . load (), en_core_web_sm . load () # STEP 1: TEXT SEGMENTATION src_segments , source_ids_map = text_segments ( queries ) trg_segments , target_ids_map = text_segments ( target ) # 2.1 INDEXING THE SOURCE src_index , src_vectors , src_indexed_terms_per_doc = text_indexer ( src_segments , stop_words = stop_words , language = language , normalize_text = normalise_text , normalize_number = normalize_number , preprocess = preprocess , find_abbreviated = find_abbreviated , logarithm = logarithm , boost = boost , remove_numbers = remove_number ) # 2.2 INDEXING THE TARGET trg_index , trg_vectors , trg_indexed_terms_per_doc = text_indexer ( trg_segments , stop_words = stop_words , language = language , normalize_text = normalise_text , normalize_number = normalize_number , preprocess = preprocess , find_abbreviated = find_abbreviated , logarithm = logarithm , boost = boost , remove_numbers = remove_number , build_tfidf = True ) # 3. UPDATE THE SOURCE'S INDEXES WITH CANDIDATES # For each indexed term in the source index, find a maximum of 5 candidate terms in the target index find_candidates ( src_index , trg_index , max_candidates = max_candidates , boost = boost ) # 4. FOR A GIVEN SOURCE-SEGMENT FIND ONE OR MORE SIMILAR TARGET-SEGMENTS results = find_match ( src_index , trg_index , trg_vectors , src_segments , trg_segments , src_indexed_terms_per_doc , x_best_tf_idfs , x_bests = bests , no_ties = no_ties ) return results STEP 1: Segmenting \u00b6 The ideal input scenario for \u201ctext-segment\u201d matching with the proposed library is for the source and target data to each be provided as a list of segments . However, if the data is provided as plain text instead, our basic text segmentation function in the library converts the plain text-document into a list of lines partitioned based on the \\n character as segments. In short, for now, the default implementation is a line-based segmentation if the input comes as plain text. We expect in the future to enable other types of segmentation such as phrase-based or paragraph-based segmentations . Naturally, for the algorithm to produce sensible results, the source and target segment-sets should follow the same segmentation criteria, or should at least be minimally compatible. For example, source segmented as phrases and target segmented as lines is not ideal, but can still provide meaningful results. STEP 2: Indexing \u00b6 The function iterates over each segment from the segments input list. First , if the preprocess argument is set as true (default), the fix_text_segment function from ftfy is applied over each input segments. This provides some fixes for inconsistencies and glitches. Then , if the normalize_number parameter is set to true (default), the numbers in the segments are converted into words using regular expression to detect numbers within the segment and the python library num2words to convert the numbers into word(s) according to the chosen language. Moreover, if the normalize_text parameter is set as true (default) and language is set to dutch, a language normalisation is applied to the segments using the following list of tuples (in future versions this list could be provided as parameter) : ('qu', 'kw'), ('sch', 'se'), ('ks', 'x'), ('kx', 'x'), ('kc', 'k'), ('ck', 'k'), ('dt', 't'), ('td', 't'), ('ch', 'g'), ('sz', 's'), ('ij', 'y') Finally , we run spacy over the fixed segment for language based tokenisation. Then, for each token, its inclusion in the index is ruled by (i) not-punctuation; (ii) not-stop-word, if such list is provided; (ii) not-number-like, if normalize_number and/or remove_numbers is set as true (default). If these conditions are passed, the lowercased version of the token is indexed, and property values of the term are documented accordingly. STEP 3: Candidate Terms \u00b6 1. Approximations to a query-term The input term (query-term) is of type string and represents either a full term or an abbreviated term. The task here of finding m candidate-terms ( m given by the max_candidates parameter, defaulted to 5 ) for the each of the given query-terms is performed using our fuzzy_match function implemented on top the fuzzywuzzy library. For a given query-term, fuzzy_match searches for at least m candidates among the indexed target terms. Only the ones with a minimum matching strength of 60 are returned. The function returns a dictionary of candidate-terms (abbreviated or not) with their respective matching-strength. 2. Candidates strength boosting The strengths of the respective candidate-terms previously selected can be boosted based on how similar their respective lemmas (and/or soundex) are to the corresponding query-terms, if the boost parameter is set as true (default). Furthermore, the boost is only applied to candidates for which the strength is below 1. For these relatively \u201cweak terms\u201d, the strength obtained by comparing lemmas and/or soundex is averaged with the approximation strength. If the resulting new strength is above a preset threshold of 75 , an average of the original strength and the current one is computed otherwise, it is left unmodified. 3. Abbreviations or extensions to a query token Roughly, terms are considered possible abbreviations if they are followed by a dot that has not been classified as punctuation by spacy . The input here is the panda\u2019s frame generated in the previous section. The idea here is to find within the frame potential extensions (full tokens) for an abbreviated token (shortened form of a token). For example, the full token civic can extend the abbreviated cvc. whereas civilian does not. The goal is to speed up the (later) process of matching possible abbreviations, if the parameter find_abbreviated is set as true (default). STEP 4: Candidate Segments. \u00b6 1. Collect candidates-terms for the query-terms After the indexing of both source and target, each entry in the source index is populated with m=5 candidates from the target index as described in step 3. This allows us to quickly retrieve candidates for each of the query-terms. For example, given a query-segment with 6 terms and max_candidates set to 5, we can quickly retrieve 30 maximum candidate-terms from various target-segments that will be further evaluated in combination. 2. Retrieve all candidate-segments based of their tf-idf A tf-idf matrix computed for the target segments is filtered based on the previously collected candidate-terms, such that it contains only the documents/segments in which at least one of the candidate-terms occur. The candidate-segments are then sorted in descending order based on the sum of their respective terms\u2019 tf-idf weights, meaning the higher the score the better. We then select the n first best candidate-segments , according to the parameter n\\_best\\_tf\\_idfs (default 50 ). 3. Compute the number of hits obtained per document At this step, for each of the n candidate-segments , we sum up the strength of its composing candidate-terms. This gives an idea of the overall matching strength for a given candidate-segment. Then, we compute their delta-hit, i.e. the difference between the maximum possible hit and the observed hit. Consequently, the candidates are reordered in ascending order based on their delta-hits, to which extra penalty and rewards are applied. The reward counts the number of intersecting terms between source and target , while the penalty looks at the offset length of the target candidate-segment with respect to the query-segment . Now, here, the smaller the score the better, since small delta implies being closer to the goal. In the end, we return the candidate-segment(s) according to the parameterised argument n_bests (default set to 1) . In the default setting, the function returns the first best even in the event of ties. If the value assigned to n_bests is greater than 1 , the function returns the number of candidates requested. However, in case of ties, it returns all of them even if the requested number is less than the number of ties.","title":"6.Algorithms"},{"location":"06.Algorithms/#6-algorithms","text":"Given source-segment and target-segment databases, we have designed and developed a library that outputs a number of candidates (of choice) from the target data for each source-segment entry. The TeAM algorithm allows for a number of other choices including among athers boosting the strength of a matched term using its lemma or soundex and/or matching possible abbreviations. To achieve this, we successfully derive benefit from existing libraries such as panda , fuzzywuzzy , spacy and ftfy . Moreover, we compare our approach with a rather simpler one mostly based on Gensim . Hereby we explain them both.","title":" 6. Algorithms "},{"location":"06.Algorithms/#61-gensim","text":"In order to have a baseline for comparing the TeAM approach, we chose the \u2018of the shelf\u2019 library \u2018Gensim\u2019 proposed for text search. We implement it first by strictly following the instructions presented in here . Since it is meant for exact term-match and it is, therefore, unable to deal with non existing (exact) terms in the target data, the results at first appear quite poor for our complex data. One way to help dealing with such issue is to convert the source and target data into their respective stems . This indeed improves the results since the matches are now approximated via the stems. Still, this does not account for misspellings or spelling variations. An attempt to address the latter issue led us to multiply the original query into a number of reconstructed queries from which the best result is selected. To reconstruct a query, n approximations are computed for each stemmed term in the query. From this, new queries are generated (term combinations by n-ary Cartesian product). For example, a query of 5 terms will result in 32 new queries if 2 candidates are found for each term. This obviously does not scale in the event of (i) big queries and/or (ii) large candidates (even 2). However, the use of a single candidate does scale and provides at list one alternative to terms that do not exist in the target data, meaning that misspell or spelling variations are now included to a certain extent. To summarise, using the Gensim library we have developed and implemented a text matching approach that now includes the search of terms that do not exist in the target data.","title":" 6.1 Gensim+ "},{"location":"06.Algorithms/#62-team","text":"This section presents TeAM, the proposed algorithm for text matching. First its parameters are described, then its 4 macro steps are explained.","title":" 6.2 TeAM"},{"location":"06.Algorithms/#function-definition","text":"Function Definition results = TeAM.run( queries=montias_data, target=inventories, stop_words=dutch_stop_word, language='nl', max_candidates=5, boost=True, preprocess=True, normalise=True, find_abbreviated=True, logarithm=False, remove_number=True, no_ties=True, x_best_tf_idfs=50, bests=1, sample=0) Parameter Description :param queries : The source data. It can be a plain text or a list of text segments. :param target : The target data. It can be a plain text or a list of text segments. :param stop_words : List of current terms that should not be indexed. :param language : A string value determining the language (e.g en, de...) in which the source and target data are written. :param max_candidates : The number of candidates terms that can be return for a given query-token :param boost : A boolean value for deciding whether to boost the strength of a matched candidate below a low-bound of 75 using the sound and lemma of the terms. :param preprocess : A boolean value for deciding whether to apply the fix_text_segment function for fixing inconsistencies and glitches in the source and target data :param normalise_text : Defaulted to [False], it is applied to the dutch written source and target data to normalise certain characters. For example, 'sch' and 'kx' will become respectfully 'see' and 'xx'. :param normalize_number : Here too, It is also used to determine whether to convert numbers to words using num2words. :param logarithm : Defaulted to [False], it uses the log function in computing tf-idf if set to [True] :param find_abbreviated : Defaulted to True, it ensures that abbreviated terms are recognised, tagged and that, possible terms for which the abbreviated term can stand for within the data are looked for. :param remove_number : Defaulted to True, it ensures that numbers are not indexed :param no_ties : Defaulted to [True].. :param x_best_tf_idfs : Defaulted to 50. The tf_idf vector enabled us to detect potential segment candidates. The integer value indicates the number of best candidates to look into. :param bests : By default, for a given query, the predicted best is returned (bests=1). :param sample : If the integer value is greater than 0, it prints panda tables of the index source, target and tf_idf vector of the target data :return : a dictionary of list or results for each query segment","title":" Function Definition "},{"location":"06.Algorithms/#overview","text":"Steps Overview def run ( queries : ( str , list ), target : ( str , list ), stop_words : list , language : str = 'nl' , preprocess : bool = True , normalise_text : bool = True , normalize_number : bool = True , remove_number = True , find_abbreviated = True , no_ties : bool = True , max_candidates : int = 5 , boost_candidates : bool = True , logarithm = False , n_best_tf_idfs : int = 50 , n_bests : int = 1 ): global nl , en nl , en = nl_core_news_sm . load (), en_core_web_sm . load () # STEP 1: TEXT SEGMENTATION src_segments , source_ids_map = text_segments ( queries ) trg_segments , target_ids_map = text_segments ( target ) # 2.1 INDEXING THE SOURCE src_index , src_vectors , src_indexed_terms_per_doc = text_indexer ( src_segments , stop_words = stop_words , language = language , normalize_text = normalise_text , normalize_number = normalize_number , preprocess = preprocess , find_abbreviated = find_abbreviated , logarithm = logarithm , boost = boost , remove_numbers = remove_number ) # 2.2 INDEXING THE TARGET trg_index , trg_vectors , trg_indexed_terms_per_doc = text_indexer ( trg_segments , stop_words = stop_words , language = language , normalize_text = normalise_text , normalize_number = normalize_number , preprocess = preprocess , find_abbreviated = find_abbreviated , logarithm = logarithm , boost = boost , remove_numbers = remove_number , build_tfidf = True ) # 3. UPDATE THE SOURCE'S INDEXES WITH CANDIDATES # For each indexed term in the source index, find a maximum of 5 candidate terms in the target index find_candidates ( src_index , trg_index , max_candidates = max_candidates , boost = boost ) # 4. FOR A GIVEN SOURCE-SEGMENT FIND ONE OR MORE SIMILAR TARGET-SEGMENTS results = find_match ( src_index , trg_index , trg_vectors , src_segments , trg_segments , src_indexed_terms_per_doc , x_best_tf_idfs , x_bests = bests , no_ties = no_ties ) return results","title":" Overview "},{"location":"06.Algorithms/#step-1-segmenting","text":"The ideal input scenario for \u201ctext-segment\u201d matching with the proposed library is for the source and target data to each be provided as a list of segments . However, if the data is provided as plain text instead, our basic text segmentation function in the library converts the plain text-document into a list of lines partitioned based on the \\n character as segments. In short, for now, the default implementation is a line-based segmentation if the input comes as plain text. We expect in the future to enable other types of segmentation such as phrase-based or paragraph-based segmentations . Naturally, for the algorithm to produce sensible results, the source and target segment-sets should follow the same segmentation criteria, or should at least be minimally compatible. For example, source segmented as phrases and target segmented as lines is not ideal, but can still provide meaningful results.","title":" STEP 1: Segmenting "},{"location":"06.Algorithms/#step-2-indexing","text":"The function iterates over each segment from the segments input list. First , if the preprocess argument is set as true (default), the fix_text_segment function from ftfy is applied over each input segments. This provides some fixes for inconsistencies and glitches. Then , if the normalize_number parameter is set to true (default), the numbers in the segments are converted into words using regular expression to detect numbers within the segment and the python library num2words to convert the numbers into word(s) according to the chosen language. Moreover, if the normalize_text parameter is set as true (default) and language is set to dutch, a language normalisation is applied to the segments using the following list of tuples (in future versions this list could be provided as parameter) : ('qu', 'kw'), ('sch', 'se'), ('ks', 'x'), ('kx', 'x'), ('kc', 'k'), ('ck', 'k'), ('dt', 't'), ('td', 't'), ('ch', 'g'), ('sz', 's'), ('ij', 'y') Finally , we run spacy over the fixed segment for language based tokenisation. Then, for each token, its inclusion in the index is ruled by (i) not-punctuation; (ii) not-stop-word, if such list is provided; (ii) not-number-like, if normalize_number and/or remove_numbers is set as true (default). If these conditions are passed, the lowercased version of the token is indexed, and property values of the term are documented accordingly.","title":" STEP 2: Indexing "},{"location":"06.Algorithms/#step-3-candidate-terms","text":"1. Approximations to a query-term The input term (query-term) is of type string and represents either a full term or an abbreviated term. The task here of finding m candidate-terms ( m given by the max_candidates parameter, defaulted to 5 ) for the each of the given query-terms is performed using our fuzzy_match function implemented on top the fuzzywuzzy library. For a given query-term, fuzzy_match searches for at least m candidates among the indexed target terms. Only the ones with a minimum matching strength of 60 are returned. The function returns a dictionary of candidate-terms (abbreviated or not) with their respective matching-strength. 2. Candidates strength boosting The strengths of the respective candidate-terms previously selected can be boosted based on how similar their respective lemmas (and/or soundex) are to the corresponding query-terms, if the boost parameter is set as true (default). Furthermore, the boost is only applied to candidates for which the strength is below 1. For these relatively \u201cweak terms\u201d, the strength obtained by comparing lemmas and/or soundex is averaged with the approximation strength. If the resulting new strength is above a preset threshold of 75 , an average of the original strength and the current one is computed otherwise, it is left unmodified. 3. Abbreviations or extensions to a query token Roughly, terms are considered possible abbreviations if they are followed by a dot that has not been classified as punctuation by spacy . The input here is the panda\u2019s frame generated in the previous section. The idea here is to find within the frame potential extensions (full tokens) for an abbreviated token (shortened form of a token). For example, the full token civic can extend the abbreviated cvc. whereas civilian does not. The goal is to speed up the (later) process of matching possible abbreviations, if the parameter find_abbreviated is set as true (default).","title":" STEP 3: Candidate Terms"},{"location":"06.Algorithms/#step-4-candidate-segments","text":"1. Collect candidates-terms for the query-terms After the indexing of both source and target, each entry in the source index is populated with m=5 candidates from the target index as described in step 3. This allows us to quickly retrieve candidates for each of the query-terms. For example, given a query-segment with 6 terms and max_candidates set to 5, we can quickly retrieve 30 maximum candidate-terms from various target-segments that will be further evaluated in combination. 2. Retrieve all candidate-segments based of their tf-idf A tf-idf matrix computed for the target segments is filtered based on the previously collected candidate-terms, such that it contains only the documents/segments in which at least one of the candidate-terms occur. The candidate-segments are then sorted in descending order based on the sum of their respective terms\u2019 tf-idf weights, meaning the higher the score the better. We then select the n first best candidate-segments , according to the parameter n\\_best\\_tf\\_idfs (default 50 ). 3. Compute the number of hits obtained per document At this step, for each of the n candidate-segments , we sum up the strength of its composing candidate-terms. This gives an idea of the overall matching strength for a given candidate-segment. Then, we compute their delta-hit, i.e. the difference between the maximum possible hit and the observed hit. Consequently, the candidates are reordered in ascending order based on their delta-hits, to which extra penalty and rewards are applied. The reward counts the number of intersecting terms between source and target , while the penalty looks at the offset length of the target candidate-segment with respect to the query-segment . Now, here, the smaller the score the better, since small delta implies being closer to the goal. In the end, we return the candidate-segment(s) according to the parameterised argument n_bests (default set to 1) . In the default setting, the function returns the first best even in the event of ties. If the value assigned to n_bests is greater than 1 , the function returns the number of candidates requested. However, in case of ties, it returns all of them even if the requested number is less than the number of ties.","title":" STEP 4: Candidate Segments. "},{"location":"07.Tests/","text":"7. TESTS. \u00b6 In this section we present the results of the conducted experiments. First, we compare results using both approaches previously described. Then we present results obtained by using different settings for the TeAM approach. For all experiments we use stop-words for Dutch written text as provided below. They were obtained here , except for the terms \u2018noch\u2019, \u2018den and \u2018vant\u2019 which were added by us. Stop-words dutch_stop_words = ['aan', 'af', 'al', 'alles', 'als', 'altijd', 'andere', 'ben', 'bij', 'daar', 'dan', 'dat', 'de', 'der', 'deze', 'die', 'dit', 'doch', 'doen', 'door', 'dus', 'een', 'eens', 'en', 'er', 'ge', 'geen', 'geweest', 'haar', 'had', 'heb', 'hebben', 'heeft', 'hem', 'het', 'hier', 'hij', 'hoe', 'hun', 'iemand', 'iets', 'ik', 'in', 'is', 'ja', 'je', 'kan', 'kon', 'kunnen', 'maar', 'me', 'meer', 'men', 'met', 'mij', 'mijn', 'moet', 'na', 'naar', 'niet', 'niets', 'nog', 'nu', 'of', 'om', 'omdat', 'ons', 'ook', 'op', 'over', 'reeds', 'te', 'tegen', 'toch', 'toen', 'tot', 'u', 'uit', 'uw', 'van', 'veel', 'voor', 'want', 'waren', 'was', 'wat', 'we', 'wel', 'werd', 'wezen', 'wie', 'wij', 'wil', 'worden', 'zal', 'ze', 'zei', 'zelf', 'zich', 'zij', 'zijn', 'zo', 'zonder', 'zou', 'noch', 'den', 'vant'] 7.1 Gensim+ versus TeAM \u00b6 In this section, we compare both approaches by increasing the parameter that defines the number m of maximum candidate approximations allowed for each term in a query. This important feature could have a big impact in the algorithm for its combinatorial nature, i.e. in principle the 0<=n<=m candidates for each term need to be combined. Results from the table present the execution times and the F 1 evaluation measures given an m value in the interval [1, 10]. The table shows that the proposed approach (TeAM) outperforms our implementation of Gensim++, which at first appears to be fast in execution time. The success of TeAM approach is specially because the combinatorial nature of the aforementioned feature is addressed in such a way that it does not affect the performance, in spite of all the extra features offered compared to the simple approach based on Gensim++. In this experiment, TeAM is at it best (F 1 =0.95) with both 6 and 7 candidates-options for a given term in the input query. Candidates Per Term Gensim++ TeAM - Best 50 TF_IDF TeAM - BEST 100 TF_IDF 1 0:00:36.943996 (0.870) 0:00:39.220609 (0.890) 0:00:42.995636 (0.890) 2 0:01:39.538036 (0.768) 0:00:39.690304 (0.900) 0:00:42.633038 (0.900) 3 \u2014 0:00:40.725126 (0.925) 0:00:42.508065 (0.920) 4 \u2014 0:00:39.612479 (0.945) 0:00:43.408359 (0.940) 5 \u2014 0:00:39.980332 (0.945) 0:00:42.566867 (0.940) 6 \u2014 0:00:40.150912 (0.950) 0:00:42.716441 (0.940) 7 \u2014 0:00:40.094593 (0.950) 0:00:43.328592 (0.935) 8 \u2014 0:00:39.921235 (0.950) 0:00:43.198330 (0.940) 9 \u2014 0:00:39.811426 (0.935) 10 \u2014 0:00:40.319461 (0.930) Result Discussion \u00b6 The observation is that the increase of the number of candidates per query-term or the number of best tf_idfs candidates-segments impacts negatively the quality of the results produced by TeAM once passed its optimum. This contradicts the our expectation. However, as the table above shows, pass height candidates per query-term, the F 1 score starts decreasing. The right answer is not always the best match which explains the fluctuation of the F 1 scores. This feature vector favours rare words which means flushing out documents with non common misspellings for example Neederlandsche histoorien get a much higher tf_idf compared to Nederlandse historien because correctly writing Nederlandse and historien is more frequent that the misspells. To compensate for that, our final choice for the best candidate(s) depends on \u201cthe best one(s)\u201d (i) being in the pool of best td_idfs and (ii) having the best hit. TEST-1: Default Settings \u00b6 The first experiment uses the default setting of each parameter, as shown in the code bellow, followed by the result of the execution. The obtained F 1 measure is 0.945 and the execution time is about 40s. Default Settings results = TeAM.run( queries=source_data, target=target_data, stop_words=TeAM.dutch_stop_word, language=language, preprocess=True, normalise_text=True, normalize_number=True, remove_number=True, find_abbreviated=True, boost_candidates=True, logarithm=False, ties=False, max_candidates=7, n_best_tf_idfs=50, n_bests=1, threshold=0) Default Settings Results 1. Segmenting the source and target Text 200 | 4179 2.1 Indexing the sources segments 0:00:03.035731 257 .................petrus................. in line 200 / 200 1. So far with a dictionary of 257 indexes 0:00:01.639557 2. So far with Frames 0:00:01.641369 3. So far with abbreviation 0:00:01.644959 2.2 Indexing the targets segments 0:00:04.680766 3617 ...............beukelaere............... in line 4178 / 4179 1. So far with a dictionary of 3617 indexes 0:00:29.802926 -> 3617 / 3617 beukelaere 0:00:00.034006 2. So far with Frames 0:00:29.845476 3. So far with abbreviation 0:00:29.900189 3. Finding 5 candidates per sources term 0:00:34.581303 257 ................petrus................. 0:00:05.248847 [Approx: 5 s] [Boost: 5 s] [Abbrev: 5 s] 4. Given a src-seg find a similar trg-seg 0:00:39.849065 Document [..199] een cleijn bortje met een ebben lijstje van den apostel Petrus 5. It took a total of 0:00:40.736017 ----------------------------------- | 4179 GROUND TRUTHS | ----------------------------------- | GT Positive | GT Negative | | 200 | 3979 | --------------------------------------------------------------------------------------------------------- | Positive | True Positive | False Positive | Precision | False discovery rate (FDR) | | 200 | 189 | 11 | 0.945 | 0.055 | PREDICT ------------------------------------------------------------------------------------------------- 4179 | Negative | False Negative | True Negative | False omission rate | Negative predictive value | | 3979 | 11 | 3968 | 0.003 | 0.997 | --------------------------------------------------------------------------------------------------------- | Recall | Fall-out | P. Likelihood Ratio | F1 score Accuracy | | 0.945 | 0.003 | 315.0 | 0.945 0.995 | -------------------------------------------------------------------------------------- TEST-2: stop_words=None \u00b6 In this experiment, the result shows that not applying stop words appear detrimental to the performance of TeAM compared to the default settings, producing F 1 measure 0.925 against the original 0.945 with a negligible execution time difference. Stop_words=None ----------------------------------- | 4179 GROUND TRUTHS | ----------------------------------- | GT Positive | GT Negative | | 200 | 3979 | --------------------------------------------------------------------------------------------------------- | Positive | True Positive | False Positive | Precision | False discovery rate (FDR) | | 200 | 185 | 15 | 0.925 | 0.075 | PREDICT ------------------------------------------------------------------------------------------------- 4179 | Negative | False Negative | True Negative | False omission rate | Negative predictive value | | 3979 | 15 | 3964 | 0.004 | 0.996 | --------------------------------------------------------------------------------------------------------- | Recall | Fall-out | P. Likelihood Ratio | F1 score Accuracy | | 0.925 | 0.004 | 231.25 | 0.925 0.993 | -------------------------------------------------------------------------------------- TEST-3: boost=False \u00b6 Here again, compared to the default settings the result shows that not applying candidate boost appears detrimental to the performance of TeAM, producing F 1 measure 0.925 against the original 0.945 with a negligible execution time difference. Boost=False ----------------------------------- | 4179 GROUND TRUTHS | ----------------------------------- | GT Positive | GT Negative | | 200 | 3979 | --------------------------------------------------------------------------------------------------------- | Positive | True Positive | False Positive | Precision | False discovery rate (FDR) | | 200 | 185 | 15 | 0.925 | 0.075 | PREDICT ------------------------------------------------------------------------------------------------- 4179 | Negative | False Negative | True Negative | False omission rate | Negative predictive value | | 3979 | 15 | 3964 | 0.004 | 0.996 | --------------------------------------------------------------------------------------------------------- | Recall | Fall-out | P. Likelihood Ratio | F1 score Accuracy | | 0.925 | 0.004 | 231.25 | 0.925 0.993 | -------------------------------------------------------------------------------------- TEST-4: preprocess=False \u00b6 Removing the text preprocessing consisting in fixes for inconsistencies and glitches appear to not affect TeAM\u2019s result for this experiment. This may suggest that the used data may be highly free from inconsistencies and glitches. Here, the result show similar F 1 score of 0.945 compared to the original score. Since having clean data may not always be true, and the difference in execution time seem negligible, this feature is enabled by default. preprocess=False ----------------------------------- | 4179 GROUND TRUTHS | ----------------------------------- | GT Positive | GT Negative | | 200 | 3979 | --------------------------------------------------------------------------------------------------------- | Positive | True Positive | False Positive | Precision | False discovery rate (FDR) | | 200 | 189 | 11 | 0.945 | 0.055 | PREDICT ------------------------------------------------------------------------------------------------- 4179 | Negative | False Negative | True Negative | False omission rate | Negative predictive value | | 3979 | 11 | 3968 | 0.003 | 0.997 | --------------------------------------------------------------------------------------------------------- | Recall | Fall-out | P. Likelihood Ratio | F1 score Accuracy | | 0.945 | 0.003 | 315.0 | 0.945 0.995 | -------------------------------------------------------------------------------------- TEST-5: normalise=False \u00b6 Compared to the default settings, the result shows that not applying text normalisation appears detrimental to the performance of TeAM in this experiment, producing an F 1 score of 0.93 against the original 0.945. The execution time difference is also negligible. To keep i mind, the current code provides only dutch normalisation. normalise=False ----------------------------------- | 4179 GROUND TRUTHS | ----------------------------------- | GT Positive | GT Negative | | 200 | 3979 | --------------------------------------------------------------------------------------------------------- | Positive | True Positive | False Positive | Precision | False discovery rate (FDR) | | 200 | 186 | 14 | 0.93 | 0.07 | PREDICT ------------------------------------------------------------------------------------------------- 4179 | Negative | False Negative | True Negative | False omission rate | Negative predictive value | | 3979 | 14 | 3965 | 0.004 | 0.996 | --------------------------------------------------------------------------------------------------------- | Recall | Fall-out | P. Likelihood Ratio | F1 score Accuracy | | 0.93 | 0.004 | 232.5 | 0.93 0.993 | -------------------------------------------------------------------------------------- TEST-6: find_abbreviated=False \u00b6 Finding abbreviations helps approximating an abbreviated term to its fullest form eiter on the source or target data. This is expected to help matching documents known to have abbreviations. ==However, the results of this experiment show that it does not have an impact in our data. This means that the matching of the non-abbreviated terms are enough to find the right segment match. Since this may not always be true, and the difference in execution time seem negligible, this feature is enabled by default. find_abbreviated =False ----------------------------------- | 4179 GROUND TRUTHS | ----------------------------------- | GT Positive | GT Negative | | 200 | 3979 | --------------------------------------------------------------------------------------------------------- | Positive | True Positive | False Positive | Precision | False discovery rate (FDR) | | 200 | 189 | 11 | 0.945 | 0.055 | PREDICT ------------------------------------------------------------------------------------------------- 4179 | Negative | False Negative | True Negative | False omission rate | Negative predictive value | | 3979 | 11 | 3968 | 0.003 | 0.997 | --------------------------------------------------------------------------------------------------------- | Recall | Fall-out | P. Likelihood Ratio | F1 score Accuracy | | 0.945 | 0.003 | 315.0 | 0.945 0.995 | TEST-7: normalize_number, remove_number=False \u00b6 In general, we expect the removal of number to be a good practice specially if it provides no adding value to the search. In doing so, it enables a decrease in space usage and computation power. As the result shows, in our experiment, removing or normalising numbers does not impact the result. Though, the good thing about keeping numbers is that it may help untying matches if relevant. normalize_number, remove_number=False ----------------------------------- | 4179 GROUND TRUTHS | ----------------------------------- | GT Positive | GT Negative | | 200 | 3979 | --------------------------------------------------------------------------------------------------------- | Positive | True Positive | False Positive | Precision | False discovery rate (FDR) | | 200 | 189 | 11 | 0.945 | 0.055 | PREDICT ------------------------------------------------------------------------------------------------- 4179 | Negative | False Negative | True Negative | False omission rate | Negative predictive value | | 3979 | 11 | 3968 | 0.003 | 0.997 | --------------------------------------------------------------------------------------------------------- | Recall | Fall-out | P. Likelihood Ratio | F1 score Accuracy | | 0.945 | 0.003 | 315.0 | 0.945 0.995 | --------------------------------------------------------------------------------------","title":"7. Tests"},{"location":"07.Tests/#7-tests","text":"In this section we present the results of the conducted experiments. First, we compare results using both approaches previously described. Then we present results obtained by using different settings for the TeAM approach. For all experiments we use stop-words for Dutch written text as provided below. They were obtained here , except for the terms \u2018noch\u2019, \u2018den and \u2018vant\u2019 which were added by us. Stop-words dutch_stop_words = ['aan', 'af', 'al', 'alles', 'als', 'altijd', 'andere', 'ben', 'bij', 'daar', 'dan', 'dat', 'de', 'der', 'deze', 'die', 'dit', 'doch', 'doen', 'door', 'dus', 'een', 'eens', 'en', 'er', 'ge', 'geen', 'geweest', 'haar', 'had', 'heb', 'hebben', 'heeft', 'hem', 'het', 'hier', 'hij', 'hoe', 'hun', 'iemand', 'iets', 'ik', 'in', 'is', 'ja', 'je', 'kan', 'kon', 'kunnen', 'maar', 'me', 'meer', 'men', 'met', 'mij', 'mijn', 'moet', 'na', 'naar', 'niet', 'niets', 'nog', 'nu', 'of', 'om', 'omdat', 'ons', 'ook', 'op', 'over', 'reeds', 'te', 'tegen', 'toch', 'toen', 'tot', 'u', 'uit', 'uw', 'van', 'veel', 'voor', 'want', 'waren', 'was', 'wat', 'we', 'wel', 'werd', 'wezen', 'wie', 'wij', 'wil', 'worden', 'zal', 'ze', 'zei', 'zelf', 'zich', 'zij', 'zijn', 'zo', 'zonder', 'zou', 'noch', 'den', 'vant']","title":" 7. TESTS. "},{"location":"07.Tests/#71-gensim-versus-team","text":"In this section, we compare both approaches by increasing the parameter that defines the number m of maximum candidate approximations allowed for each term in a query. This important feature could have a big impact in the algorithm for its combinatorial nature, i.e. in principle the 0<=n<=m candidates for each term need to be combined. Results from the table present the execution times and the F 1 evaluation measures given an m value in the interval [1, 10]. The table shows that the proposed approach (TeAM) outperforms our implementation of Gensim++, which at first appears to be fast in execution time. The success of TeAM approach is specially because the combinatorial nature of the aforementioned feature is addressed in such a way that it does not affect the performance, in spite of all the extra features offered compared to the simple approach based on Gensim++. In this experiment, TeAM is at it best (F 1 =0.95) with both 6 and 7 candidates-options for a given term in the input query. Candidates Per Term Gensim++ TeAM - Best 50 TF_IDF TeAM - BEST 100 TF_IDF 1 0:00:36.943996 (0.870) 0:00:39.220609 (0.890) 0:00:42.995636 (0.890) 2 0:01:39.538036 (0.768) 0:00:39.690304 (0.900) 0:00:42.633038 (0.900) 3 \u2014 0:00:40.725126 (0.925) 0:00:42.508065 (0.920) 4 \u2014 0:00:39.612479 (0.945) 0:00:43.408359 (0.940) 5 \u2014 0:00:39.980332 (0.945) 0:00:42.566867 (0.940) 6 \u2014 0:00:40.150912 (0.950) 0:00:42.716441 (0.940) 7 \u2014 0:00:40.094593 (0.950) 0:00:43.328592 (0.935) 8 \u2014 0:00:39.921235 (0.950) 0:00:43.198330 (0.940) 9 \u2014 0:00:39.811426 (0.935) 10 \u2014 0:00:40.319461 (0.930)","title":"7.1 Gensim+ versus TeAM"},{"location":"07.Tests/#result-discussion","text":"The observation is that the increase of the number of candidates per query-term or the number of best tf_idfs candidates-segments impacts negatively the quality of the results produced by TeAM once passed its optimum. This contradicts the our expectation. However, as the table above shows, pass height candidates per query-term, the F 1 score starts decreasing. The right answer is not always the best match which explains the fluctuation of the F 1 scores. This feature vector favours rare words which means flushing out documents with non common misspellings for example Neederlandsche histoorien get a much higher tf_idf compared to Nederlandse historien because correctly writing Nederlandse and historien is more frequent that the misspells. To compensate for that, our final choice for the best candidate(s) depends on \u201cthe best one(s)\u201d (i) being in the pool of best td_idfs and (ii) having the best hit.","title":" Result Discussion "},{"location":"07.Tests/#test-1-default-settings","text":"The first experiment uses the default setting of each parameter, as shown in the code bellow, followed by the result of the execution. The obtained F 1 measure is 0.945 and the execution time is about 40s. Default Settings results = TeAM.run( queries=source_data, target=target_data, stop_words=TeAM.dutch_stop_word, language=language, preprocess=True, normalise_text=True, normalize_number=True, remove_number=True, find_abbreviated=True, boost_candidates=True, logarithm=False, ties=False, max_candidates=7, n_best_tf_idfs=50, n_bests=1, threshold=0) Default Settings Results 1. Segmenting the source and target Text 200 | 4179 2.1 Indexing the sources segments 0:00:03.035731 257 .................petrus................. in line 200 / 200 1. So far with a dictionary of 257 indexes 0:00:01.639557 2. So far with Frames 0:00:01.641369 3. So far with abbreviation 0:00:01.644959 2.2 Indexing the targets segments 0:00:04.680766 3617 ...............beukelaere............... in line 4178 / 4179 1. So far with a dictionary of 3617 indexes 0:00:29.802926 -> 3617 / 3617 beukelaere 0:00:00.034006 2. So far with Frames 0:00:29.845476 3. So far with abbreviation 0:00:29.900189 3. Finding 5 candidates per sources term 0:00:34.581303 257 ................petrus................. 0:00:05.248847 [Approx: 5 s] [Boost: 5 s] [Abbrev: 5 s] 4. Given a src-seg find a similar trg-seg 0:00:39.849065 Document [..199] een cleijn bortje met een ebben lijstje van den apostel Petrus 5. It took a total of 0:00:40.736017 ----------------------------------- | 4179 GROUND TRUTHS | ----------------------------------- | GT Positive | GT Negative | | 200 | 3979 | --------------------------------------------------------------------------------------------------------- | Positive | True Positive | False Positive | Precision | False discovery rate (FDR) | | 200 | 189 | 11 | 0.945 | 0.055 | PREDICT ------------------------------------------------------------------------------------------------- 4179 | Negative | False Negative | True Negative | False omission rate | Negative predictive value | | 3979 | 11 | 3968 | 0.003 | 0.997 | --------------------------------------------------------------------------------------------------------- | Recall | Fall-out | P. Likelihood Ratio | F1 score Accuracy | | 0.945 | 0.003 | 315.0 | 0.945 0.995 | --------------------------------------------------------------------------------------","title":"TEST-1: Default Settings"},{"location":"07.Tests/#test-2-stop_wordsnone","text":"In this experiment, the result shows that not applying stop words appear detrimental to the performance of TeAM compared to the default settings, producing F 1 measure 0.925 against the original 0.945 with a negligible execution time difference. Stop_words=None ----------------------------------- | 4179 GROUND TRUTHS | ----------------------------------- | GT Positive | GT Negative | | 200 | 3979 | --------------------------------------------------------------------------------------------------------- | Positive | True Positive | False Positive | Precision | False discovery rate (FDR) | | 200 | 185 | 15 | 0.925 | 0.075 | PREDICT ------------------------------------------------------------------------------------------------- 4179 | Negative | False Negative | True Negative | False omission rate | Negative predictive value | | 3979 | 15 | 3964 | 0.004 | 0.996 | --------------------------------------------------------------------------------------------------------- | Recall | Fall-out | P. Likelihood Ratio | F1 score Accuracy | | 0.925 | 0.004 | 231.25 | 0.925 0.993 | --------------------------------------------------------------------------------------","title":"TEST-2: stop_words=None"},{"location":"07.Tests/#test-3-boostfalse","text":"Here again, compared to the default settings the result shows that not applying candidate boost appears detrimental to the performance of TeAM, producing F 1 measure 0.925 against the original 0.945 with a negligible execution time difference. Boost=False ----------------------------------- | 4179 GROUND TRUTHS | ----------------------------------- | GT Positive | GT Negative | | 200 | 3979 | --------------------------------------------------------------------------------------------------------- | Positive | True Positive | False Positive | Precision | False discovery rate (FDR) | | 200 | 185 | 15 | 0.925 | 0.075 | PREDICT ------------------------------------------------------------------------------------------------- 4179 | Negative | False Negative | True Negative | False omission rate | Negative predictive value | | 3979 | 15 | 3964 | 0.004 | 0.996 | --------------------------------------------------------------------------------------------------------- | Recall | Fall-out | P. Likelihood Ratio | F1 score Accuracy | | 0.925 | 0.004 | 231.25 | 0.925 0.993 | --------------------------------------------------------------------------------------","title":"TEST-3: boost=False"},{"location":"07.Tests/#test-4-preprocessfalse","text":"Removing the text preprocessing consisting in fixes for inconsistencies and glitches appear to not affect TeAM\u2019s result for this experiment. This may suggest that the used data may be highly free from inconsistencies and glitches. Here, the result show similar F 1 score of 0.945 compared to the original score. Since having clean data may not always be true, and the difference in execution time seem negligible, this feature is enabled by default. preprocess=False ----------------------------------- | 4179 GROUND TRUTHS | ----------------------------------- | GT Positive | GT Negative | | 200 | 3979 | --------------------------------------------------------------------------------------------------------- | Positive | True Positive | False Positive | Precision | False discovery rate (FDR) | | 200 | 189 | 11 | 0.945 | 0.055 | PREDICT ------------------------------------------------------------------------------------------------- 4179 | Negative | False Negative | True Negative | False omission rate | Negative predictive value | | 3979 | 11 | 3968 | 0.003 | 0.997 | --------------------------------------------------------------------------------------------------------- | Recall | Fall-out | P. Likelihood Ratio | F1 score Accuracy | | 0.945 | 0.003 | 315.0 | 0.945 0.995 | --------------------------------------------------------------------------------------","title":"TEST-4: preprocess=False"},{"location":"07.Tests/#test-5-normalisefalse","text":"Compared to the default settings, the result shows that not applying text normalisation appears detrimental to the performance of TeAM in this experiment, producing an F 1 score of 0.93 against the original 0.945. The execution time difference is also negligible. To keep i mind, the current code provides only dutch normalisation. normalise=False ----------------------------------- | 4179 GROUND TRUTHS | ----------------------------------- | GT Positive | GT Negative | | 200 | 3979 | --------------------------------------------------------------------------------------------------------- | Positive | True Positive | False Positive | Precision | False discovery rate (FDR) | | 200 | 186 | 14 | 0.93 | 0.07 | PREDICT ------------------------------------------------------------------------------------------------- 4179 | Negative | False Negative | True Negative | False omission rate | Negative predictive value | | 3979 | 14 | 3965 | 0.004 | 0.996 | --------------------------------------------------------------------------------------------------------- | Recall | Fall-out | P. Likelihood Ratio | F1 score Accuracy | | 0.93 | 0.004 | 232.5 | 0.93 0.993 | --------------------------------------------------------------------------------------","title":"TEST-5: normalise=False"},{"location":"07.Tests/#test-6-find_abbreviatedfalse","text":"Finding abbreviations helps approximating an abbreviated term to its fullest form eiter on the source or target data. This is expected to help matching documents known to have abbreviations. ==However, the results of this experiment show that it does not have an impact in our data. This means that the matching of the non-abbreviated terms are enough to find the right segment match. Since this may not always be true, and the difference in execution time seem negligible, this feature is enabled by default. find_abbreviated =False ----------------------------------- | 4179 GROUND TRUTHS | ----------------------------------- | GT Positive | GT Negative | | 200 | 3979 | --------------------------------------------------------------------------------------------------------- | Positive | True Positive | False Positive | Precision | False discovery rate (FDR) | | 200 | 189 | 11 | 0.945 | 0.055 | PREDICT ------------------------------------------------------------------------------------------------- 4179 | Negative | False Negative | True Negative | False omission rate | Negative predictive value | | 3979 | 11 | 3968 | 0.003 | 0.997 | --------------------------------------------------------------------------------------------------------- | Recall | Fall-out | P. Likelihood Ratio | F1 score Accuracy | | 0.945 | 0.003 | 315.0 | 0.945 0.995 |","title":"TEST-6: find_abbreviated=False"},{"location":"07.Tests/#test-7-normalize_number-remove_numberfalse","text":"In general, we expect the removal of number to be a good practice specially if it provides no adding value to the search. In doing so, it enables a decrease in space usage and computation power. As the result shows, in our experiment, removing or normalising numbers does not impact the result. Though, the good thing about keeping numbers is that it may help untying matches if relevant. normalize_number, remove_number=False ----------------------------------- | 4179 GROUND TRUTHS | ----------------------------------- | GT Positive | GT Negative | | 200 | 3979 | --------------------------------------------------------------------------------------------------------- | Positive | True Positive | False Positive | Precision | False discovery rate (FDR) | | 200 | 189 | 11 | 0.945 | 0.055 | PREDICT ------------------------------------------------------------------------------------------------- 4179 | Negative | False Negative | True Negative | False omission rate | Negative predictive value | | 3979 | 11 | 3968 | 0.003 | 0.997 | --------------------------------------------------------------------------------------------------------- | Recall | Fall-out | P. Likelihood Ratio | F1 score Accuracy | | 0.945 | 0.003 | 315.0 | 0.945 0.995 | --------------------------------------------------------------------------------------","title":"TEST-7:  normalize_number, remove_number=False"},{"location":"08.Conclusion/","text":"8. Conclusion \u00b6","title":"8.Conclusion"},{"location":"08.Conclusion/#8-conclusion","text":"","title":"8. Conclusion"}]}